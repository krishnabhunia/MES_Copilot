{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain_groq in /home/kb/MES_Copilot/POC/venv_kri_mes/lib/python3.10/site-packages (0.2.0)\n",
      "Requirement already satisfied: groq<1,>=0.4.1 in /home/kb/MES_Copilot/POC/venv_kri_mes/lib/python3.10/site-packages (from langchain_groq) (0.11.0)\n",
      "Requirement already satisfied: langchain-core<0.4,>=0.3 in /home/kb/MES_Copilot/POC/venv_kri_mes/lib/python3.10/site-packages (from langchain_groq) (0.3.9)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /home/kb/MES_Copilot/POC/venv_kri_mes/lib/python3.10/site-packages (from groq<1,>=0.4.1->langchain_groq) (4.6.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /home/kb/MES_Copilot/POC/venv_kri_mes/lib/python3.10/site-packages (from groq<1,>=0.4.1->langchain_groq) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/kb/MES_Copilot/POC/venv_kri_mes/lib/python3.10/site-packages (from groq<1,>=0.4.1->langchain_groq) (0.27.2)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /home/kb/MES_Copilot/POC/venv_kri_mes/lib/python3.10/site-packages (from groq<1,>=0.4.1->langchain_groq) (2.9.2)\n",
      "Requirement already satisfied: sniffio in /home/kb/MES_Copilot/POC/venv_kri_mes/lib/python3.10/site-packages (from groq<1,>=0.4.1->langchain_groq) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /home/kb/MES_Copilot/POC/venv_kri_mes/lib/python3.10/site-packages (from groq<1,>=0.4.1->langchain_groq) (4.12.2)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /home/kb/MES_Copilot/POC/venv_kri_mes/lib/python3.10/site-packages (from langchain-core<0.4,>=0.3->langchain_groq) (6.0.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /home/kb/MES_Copilot/POC/venv_kri_mes/lib/python3.10/site-packages (from langchain-core<0.4,>=0.3->langchain_groq) (1.33)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in /home/kb/MES_Copilot/POC/venv_kri_mes/lib/python3.10/site-packages (from langchain-core<0.4,>=0.3->langchain_groq) (0.1.132)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /home/kb/MES_Copilot/POC/venv_kri_mes/lib/python3.10/site-packages (from langchain-core<0.4,>=0.3->langchain_groq) (24.1)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /home/kb/MES_Copilot/POC/venv_kri_mes/lib/python3.10/site-packages (from langchain-core<0.4,>=0.3->langchain_groq) (8.5.0)\n",
      "Requirement already satisfied: idna>=2.8 in /home/kb/MES_Copilot/POC/venv_kri_mes/lib/python3.10/site-packages (from anyio<5,>=3.5.0->groq<1,>=0.4.1->langchain_groq) (3.10)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/kb/MES_Copilot/POC/venv_kri_mes/lib/python3.10/site-packages (from anyio<5,>=3.5.0->groq<1,>=0.4.1->langchain_groq) (1.2.2)\n",
      "Requirement already satisfied: certifi in /home/kb/MES_Copilot/POC/venv_kri_mes/lib/python3.10/site-packages (from httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain_groq) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /home/kb/MES_Copilot/POC/venv_kri_mes/lib/python3.10/site-packages (from httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain_groq) (1.0.6)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/kb/MES_Copilot/POC/venv_kri_mes/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain_groq) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /home/kb/MES_Copilot/POC/venv_kri_mes/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4,>=0.3->langchain_groq) (3.0.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /home/kb/MES_Copilot/POC/venv_kri_mes/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3->langchain_groq) (3.10.7)\n",
      "Requirement already satisfied: requests<3,>=2 in /home/kb/MES_Copilot/POC/venv_kri_mes/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3->langchain_groq) (2.32.3)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /home/kb/MES_Copilot/POC/venv_kri_mes/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3->langchain_groq) (1.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/kb/MES_Copilot/POC/venv_kri_mes/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->groq<1,>=0.4.1->langchain_groq) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /home/kb/MES_Copilot/POC/venv_kri_mes/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->groq<1,>=0.4.1->langchain_groq) (2.23.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/kb/MES_Copilot/POC/venv_kri_mes/lib/python3.10/site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3->langchain_groq) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/kb/MES_Copilot/POC/venv_kri_mes/lib/python3.10/site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3->langchain_groq) (2.2.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymupdf in /home/kb/MES_Copilot/POC/venv_kri_mes/lib/python3.10/site-packages (1.24.11)\n"
     ]
    }
   ],
   "source": [
    "! pip install langchain_groq\n",
    "! pip install pymupdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "from langchain.agents import create_tool_calling_agent, AgentExecutor\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "from langchain_community.document_loaders import PyMuPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load documents\n",
    "pdf_loader = PyMuPDFLoader(\"krishna_resume.pdf\")\n",
    "documents = pdf_loader.load()\n",
    "\n",
    "# Split text into chunks\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "embedding_model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "embeddings = HuggingFaceEmbeddings(model_name=embedding_model_name)\n",
    "\n",
    "vector_store = FAISS.from_documents(texts, embeddings)\n",
    "retriever = vector_store.as_retriever()\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant\"),\n",
    "        (\"placeholder\", \"{chat_history}\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "        (\"placeholder\", \"{agent_scratchpad}\")\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGroq(\n",
    "    model=\"mixtral-8x7b-32768\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2\n",
    ")\n",
    "\n",
    "\n",
    "# llm = ChatGroq(model=\"llama3-70b-8192\", temperature=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def retrieve_documents(query: str) -> str:\n",
    "    \"\"\"Retrieve relevant documents based on the query\"\"\"\n",
    "    docs = retriever.invoke(query)\n",
    "    return \"\\n\".join([doc.page_content for doc in docs])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `retrieve_documents` with `{'query': 'who is Krishna'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mKRISHNA DIPAYAN BHUNIA \n",
      "Senior Software Engineer | Senior Data Engineer | Python Developer \n",
      "+91 865 200 7894 \n",
      " \n",
      "Github Link \n",
      "LinkedIn \n",
      " \n",
      "kri.career@gmail.com \n",
      "SUMMARY \n",
      "● Seasoned Software Engineer/Full Stack Developer with 12+ years of experience across industries like CRM, Oil & Gas, Financial, and FMCG. Over \n",
      "6 years of expertise in Python as a Data Engineer/Scientist, specializing in data engineering, analysis, and visualization.  \n",
      "● Proficient in writing clean, production-level Python code, with strong skills in Object-Oriented Programming, DSA, and relational databases. Used \n",
      "Python libraries such as NumPy, and Pandas for advanced data analysis and engineering stuff. \n",
      "● Experienced in designing software solutions using Python, C#, and back-end technologies. Adept in cloud platforms (AWS, Azure), containerization \n",
      "(Docker), and database management (SQL, No-SQL), with a strong ability to analyze algorithms and collaborate effectively. \n",
      "EDUCATION \n",
      "● M.TECH (Computer Eng. with Specialization in Software Eng.)   \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Aug 2016 – Jun 2018 \n",
      "Veermata Jijabai Technological Institute (VJTI), Mumbai \n",
      "● B.E Computer Engineering (University of Mumbai) \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Aug 2008 – Jun 2012 \n",
      "TECHNICAL SKILLS \n",
      "● Languages & Frameworks: Python, NumPy, Pandas, Java, JavaScript (jQuery), MS SQL, PL-SQL, Oracle, C# (C-Sharp), Docker, Github. \n",
      "● Data Engineering & Analysis: Data Science and Data engineering processes, Data Analysis, Classification, Regression Analysis, Cluster Analysis, \n",
      "Data Visualization, Statistics, AWS (Amazon Web Services), Azure. \n",
      "● Databases: MS SQL Server (2019/2017), MySQL, MongoDB, Oracle 19c. \n",
      "● Expertise: DSA (Data Structures & Algorithms), Algorithm Analysis (Space & Time Complexity), Distributed Systems & Architecture. \n",
      "WORK EXPERIENCE & PROJECTS \n",
      "Senior Data Engineer in Capgemini \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "    Oct 2021 – Present \n",
      "Client - P&G(Procter & Gamble) - (Role - Data Engineer, Developer)  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "            \n",
      "Pune(Remote) \n",
      "● Technical Stack: Python, Pandas, NumPy, MS-SQL 2022, JSON, VS Code, Azure, Azure Pipelines, Docker, GitHub. \n",
      "● Proficient in Python for data engineering pipelines. \n",
      "● Experienced with Pandas, NumPy for data processing and analysis. \n",
      "● Skilled in client communication to gather data and project requirements. \n",
      "● Set up and configured Azure pipelines for CI/CD of data pipelines. \n",
      "● Created and optimized MS-SQL tables for efficient data storage. \n",
      "● Populated metrics to Grafana Dashboard from manufacturing data. \n",
      "● Designed ETL pipelines for data extraction from various sources. \n",
      "● Applied statistical techniques and optimization for data contextualization. \n",
      "● Delivered pipelines for leakage, defect detection, and insights. \n",
      "Key Achievement - With accommodating data from multiple sources makes it easy to contextualize data for easy analysis purposes, which improves \n",
      "the manufacturing lines are more adhered to not making defects/leakages in product manufacturing. The defects/leakages have been reduced to half \n",
      "of the previous defects identified. \n",
      "Software Engineer and Data Analyst in Larsen and Toubro Technological Services worked in Oil & Gas Domain \n",
      " \n",
      "Jan 2020 – Oct 2021 \n",
      " (Role – Data Engineer, Developer) \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Vadodara \n",
      "  (Python, NumPy, Pandas, SciPy, MS SQL Server 2017, Oracle 11G and Oracle 10G, xlrd, xlwt, openpyxl (Anaconda Navigator Framework)). \n",
      "Plant Portal Data Analysis [ Covestro, IPEP (Data analytics team)] \n",
      "● Implementation for the process of Analysis (Deduplication, Data Seagration, File Reference Missing in Smart Plant portal, finding different \n",
      "equipment properties in plant portal, Different levels of data missing. Different file formats, Segregation and separation of different language \n",
      "formats, metadata, and migrated documents in smart plant portal).  \n",
      "● Used Task, Thread and Parallel process and optimized codes with Singleton and Factory Design pattern. Excel dynamic read and write with \n",
      "formatting using closedXML, OpenXML, EPPlus, Excel Data Reader. \n",
      "● Key Achievement – Computation has reduced time significantly for a large dataset and customer delivery satisfaction was achieved on-time. \n",
      "SPI (Smart Plant Integration) and SPEC [ Covestro, IPEP (Data analytics team)] \n",
      "● Data Comparison of migrating data from oracle to SQL Server. Generation of Excel Data and its comparison using EPPlus and generating reports. \n",
      "Created Custom controls and User controls, Delegation of custom and user controls, Background task and multithreading with parameterized \n",
      "arguments, async and await tasks, a comparison using multi and different primary keys also with dynamic and different key columns. Data view \n",
      "sync. Comparison checks for BLOB data and other Hash File data. \n",
      "● Implemented Task, Thread and Parallel process and optimized codes with Singleton and Factory Design pattern. Excel dynamic read and write \n",
      "with formatting using closedXML, EPPlus, ExcelDataReader and Dataset in Visual Studio. Logging facility in python and used data frames. \n",
      "● Key Achievement – With multithreading and parallel processing output time has been significantly improved by more than 90%. Worked closely \n",
      "with cross-functional teams including plant engineers, data owners and site engineers to design, build and deliver end-to end solutions to their \n",
      "individual data conversion and wrangling problems. \n",
      "Software Engineer in Quantum Mutual Fund worked in Financial Domain \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " Aug 2018 - Jan 2020 \n",
      "(Role – Full Stack Developer/ Data Engineer)  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "     Mumba \n",
      "Invest Online (INON) - Backend Developer \n",
      "● Developed business logic for investment in Q-MF & transaction, Re-Purchase, Redemption, Switch, New Purchase, STP, SWP, SIP. \n",
      "● Optimized backend SQL code for Investors, used to do KYC according to their citizenship with different criteria & process form in different queues. \n",
      "Software Engineer in Smart Solutions worked in CRM, FMCG  Domain \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Oct 2013 - July 2018 \n",
      "(Role – Full Stack Developer)  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "     Mumbai \n",
      "Inventory management and services - Backend Developer  \n",
      "● Capturing, managing, and maintaining inventories and managing. Assigning blocks and holding inventories. \n",
      "Graduate Engineer Trainee in Godrej & Boyce worked in CRM Domain \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Oct 2012 - Sept 2013 \n",
      "(Role – Full Stack Developer)  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "     Mumbai \n",
      "Timespan- Attendance and Leave management system, Assets management system & Drawer depository safe - Application  \n",
      "● Capturing, managing, and maintaining attendance and leaves for the employee in the company. Migrating assets and system information to \n",
      "standard format. \n",
      "CERTIFICATION AND LINKS (Recent) \n",
      "● LinkedIn & Pluralsight Certificates - #Using Python with Excel  #Data Wrangling with Pandas for Data Engineers\u001b[0m\u001b[32;1m\u001b[1;3mKrishna Dipayan Bhunia is a Senior Software Engineer and Data Engineer with 12+ years of experience across various industries such as CRM, Oil & Gas, Financial, and FMCG. He specializes in data engineering, analysis, and visualization with over 6 years of expertise in Python. Krishna has a strong background in writing clean, production-level Python code and is proficient in various programming languages, frameworks, and databases.\n",
      "\n",
      "He has worked on multiple projects, including his current role as a Senior Data Engineer at Capgemini for P&G, where he has been responsible for data engineering pipelines, data processing and analysis, client communication, and ETL pipelines for data extraction from various sources. Krishna has also worked as a Software Engineer and Data Analyst in the Oil & Gas domain at Larsen and Toubro Technological Services, where he implemented data analysis processes and optimized codes using various tools and libraries.\n",
      "\n",
      "Krishna has a Master of Technology degree in Computer Engineering with a specialization in Software Engineering from Veermata Jijabai Technological Institute (VJTI), Mumbai, and a Bachelor of Engineering degree in Computer Engineering from the University of Mumbai. He has also earned certifications from LinkedIn and Pluralsight for using Python with Excel and data wrangling with Pandas for Data Engineers.\n",
      "\n",
      "In summary, Krishna Dipayan Bhunia is a highly skilled and experienced Senior Software Engineer and Data Engineer with a strong background in Python and data engineering. He has a proven track record of delivering high-quality solutions and working closely with cross-functional teams to design, build, and deliver end-to-end solutions.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Retrieve relevant documents for: who is Krishna?',\n",
       " 'output': 'Krishna Dipayan Bhunia is a Senior Software Engineer and Data Engineer with 12+ years of experience across various industries such as CRM, Oil & Gas, Financial, and FMCG. He specializes in data engineering, analysis, and visualization with over 6 years of expertise in Python. Krishna has a strong background in writing clean, production-level Python code and is proficient in various programming languages, frameworks, and databases.\\n\\nHe has worked on multiple projects, including his current role as a Senior Data Engineer at Capgemini for P&G, where he has been responsible for data engineering pipelines, data processing and analysis, client communication, and ETL pipelines for data extraction from various sources. Krishna has also worked as a Software Engineer and Data Analyst in the Oil & Gas domain at Larsen and Toubro Technological Services, where he implemented data analysis processes and optimized codes using various tools and libraries.\\n\\nKrishna has a Master of Technology degree in Computer Engineering with a specialization in Software Engineering from Veermata Jijabai Technological Institute (VJTI), Mumbai, and a Bachelor of Engineering degree in Computer Engineering from the University of Mumbai. He has also earned certifications from LinkedIn and Pluralsight for using Python with Excel and data wrangling with Pandas for Data Engineers.\\n\\nIn summary, Krishna Dipayan Bhunia is a highly skilled and experienced Senior Software Engineer and Data Engineer with a strong background in Python and data engineering. He has a proven track record of delivering high-quality solutions and working closely with cross-functional teams to design, build, and deliver end-to-end solutions.'}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools = [retrieve_documents]\n",
    "query = \"who is Krishna?\"\n",
    "\n",
    "agent = create_tool_calling_agent(llm, tools, prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "agent_executor.invoke({\"input\": f\"Retrieve relevant documents for: {query}\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `retrieve_documents` with `{'query': \"Krishna's company\"}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mKRISHNA DIPAYAN BHUNIA \n",
      "Senior Software Engineer | Senior Data Engineer | Python Developer \n",
      "+91 865 200 7894 \n",
      " \n",
      "Github Link \n",
      "LinkedIn \n",
      " \n",
      "kri.career@gmail.com \n",
      "SUMMARY \n",
      "● Seasoned Software Engineer/Full Stack Developer with 12+ years of experience across industries like CRM, Oil & Gas, Financial, and FMCG. Over \n",
      "6 years of expertise in Python as a Data Engineer/Scientist, specializing in data engineering, analysis, and visualization.  \n",
      "● Proficient in writing clean, production-level Python code, with strong skills in Object-Oriented Programming, DSA, and relational databases. Used \n",
      "Python libraries such as NumPy, and Pandas for advanced data analysis and engineering stuff. \n",
      "● Experienced in designing software solutions using Python, C#, and back-end technologies. Adept in cloud platforms (AWS, Azure), containerization \n",
      "(Docker), and database management (SQL, No-SQL), with a strong ability to analyze algorithms and collaborate effectively. \n",
      "EDUCATION \n",
      "● M.TECH (Computer Eng. with Specialization in Software Eng.)   \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Aug 2016 – Jun 2018 \n",
      "Veermata Jijabai Technological Institute (VJTI), Mumbai \n",
      "● B.E Computer Engineering (University of Mumbai) \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Aug 2008 – Jun 2012 \n",
      "TECHNICAL SKILLS \n",
      "● Languages & Frameworks: Python, NumPy, Pandas, Java, JavaScript (jQuery), MS SQL, PL-SQL, Oracle, C# (C-Sharp), Docker, Github. \n",
      "● Data Engineering & Analysis: Data Science and Data engineering processes, Data Analysis, Classification, Regression Analysis, Cluster Analysis, \n",
      "Data Visualization, Statistics, AWS (Amazon Web Services), Azure. \n",
      "● Databases: MS SQL Server (2019/2017), MySQL, MongoDB, Oracle 19c. \n",
      "● Expertise: DSA (Data Structures & Algorithms), Algorithm Analysis (Space & Time Complexity), Distributed Systems & Architecture. \n",
      "WORK EXPERIENCE & PROJECTS \n",
      "Senior Data Engineer in Capgemini \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "    Oct 2021 – Present \n",
      "Client - P&G(Procter & Gamble) - (Role - Data Engineer, Developer)  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "            \n",
      "Pune(Remote) \n",
      "● Technical Stack: Python, Pandas, NumPy, MS-SQL 2022, JSON, VS Code, Azure, Azure Pipelines, Docker, GitHub. \n",
      "● Proficient in Python for data engineering pipelines. \n",
      "● Experienced with Pandas, NumPy for data processing and analysis. \n",
      "● Skilled in client communication to gather data and project requirements. \n",
      "● Set up and configured Azure pipelines for CI/CD of data pipelines. \n",
      "● Created and optimized MS-SQL tables for efficient data storage. \n",
      "● Populated metrics to Grafana Dashboard from manufacturing data. \n",
      "● Designed ETL pipelines for data extraction from various sources. \n",
      "● Applied statistical techniques and optimization for data contextualization. \n",
      "● Delivered pipelines for leakage, defect detection, and insights. \n",
      "Key Achievement - With accommodating data from multiple sources makes it easy to contextualize data for easy analysis purposes, which improves \n",
      "the manufacturing lines are more adhered to not making defects/leakages in product manufacturing. The defects/leakages have been reduced to half \n",
      "of the previous defects identified. \n",
      "Software Engineer and Data Analyst in Larsen and Toubro Technological Services worked in Oil & Gas Domain \n",
      " \n",
      "Jan 2020 – Oct 2021 \n",
      " (Role – Data Engineer, Developer) \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Vadodara \n",
      "  (Python, NumPy, Pandas, SciPy, MS SQL Server 2017, Oracle 11G and Oracle 10G, xlrd, xlwt, openpyxl (Anaconda Navigator Framework)). \n",
      "Plant Portal Data Analysis [ Covestro, IPEP (Data analytics team)] \n",
      "● Implementation for the process of Analysis (Deduplication, Data Seagration, File Reference Missing in Smart Plant portal, finding different \n",
      "equipment properties in plant portal, Different levels of data missing. Different file formats, Segregation and separation of different language \n",
      "formats, metadata, and migrated documents in smart plant portal).  \n",
      "● Used Task, Thread and Parallel process and optimized codes with Singleton and Factory Design pattern. Excel dynamic read and write with \n",
      "formatting using closedXML, OpenXML, EPPlus, Excel Data Reader. \n",
      "● Key Achievement – Computation has reduced time significantly for a large dataset and customer delivery satisfaction was achieved on-time. \n",
      "SPI (Smart Plant Integration) and SPEC [ Covestro, IPEP (Data analytics team)] \n",
      "● Data Comparison of migrating data from oracle to SQL Server. Generation of Excel Data and its comparison using EPPlus and generating reports. \n",
      "Created Custom controls and User controls, Delegation of custom and user controls, Background task and multithreading with parameterized \n",
      "arguments, async and await tasks, a comparison using multi and different primary keys also with dynamic and different key columns. Data view \n",
      "sync. Comparison checks for BLOB data and other Hash File data. \n",
      "● Implemented Task, Thread and Parallel process and optimized codes with Singleton and Factory Design pattern. Excel dynamic read and write \n",
      "with formatting using closedXML, EPPlus, ExcelDataReader and Dataset in Visual Studio. Logging facility in python and used data frames. \n",
      "● Key Achievement – With multithreading and parallel processing output time has been significantly improved by more than 90%. Worked closely \n",
      "with cross-functional teams including plant engineers, data owners and site engineers to design, build and deliver end-to end solutions to their \n",
      "individual data conversion and wrangling problems. \n",
      "Software Engineer in Quantum Mutual Fund worked in Financial Domain \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " Aug 2018 - Jan 2020 \n",
      "(Role – Full Stack Developer/ Data Engineer)  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "     Mumba \n",
      "Invest Online (INON) - Backend Developer \n",
      "● Developed business logic for investment in Q-MF & transaction, Re-Purchase, Redemption, Switch, New Purchase, STP, SWP, SIP. \n",
      "● Optimized backend SQL code for Investors, used to do KYC according to their citizenship with different criteria & process form in different queues. \n",
      "Software Engineer in Smart Solutions worked in CRM, FMCG  Domain \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Oct 2013 - July 2018 \n",
      "(Role – Full Stack Developer)  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "     Mumbai \n",
      "Inventory management and services - Backend Developer  \n",
      "● Capturing, managing, and maintaining inventories and managing. Assigning blocks and holding inventories. \n",
      "Graduate Engineer Trainee in Godrej & Boyce worked in CRM Domain \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Oct 2012 - Sept 2013 \n",
      "(Role – Full Stack Developer)  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "     Mumbai \n",
      "Timespan- Attendance and Leave management system, Assets management system & Drawer depository safe - Application  \n",
      "● Capturing, managing, and maintaining attendance and leaves for the employee in the company. Migrating assets and system information to \n",
      "standard format. \n",
      "CERTIFICATION AND LINKS (Recent) \n",
      "● LinkedIn & Pluralsight Certificates - #Using Python with Excel  #Data Wrangling with Pandas for Data Engineers\u001b[0m\u001b[32;1m\u001b[1;3mBased on the information provided by the tool, Krishna Dipayan Bhunia works as a Senior Data Engineer at Capgemini, serving the client Procter & Gamble (P&G). Krishna's work involves data engineering pipelines using Python, Pandas, NumPy, MS-SQL 2022, Azure, and Docker. One of Krishna's key achievements at Capgemini includes reducing defects and leakages in product manufacturing by half by efficiently contextualizing data from multiple sources.\n",
      "\n",
      "Previously, Krishna worked as a Software Engineer and Data Analyst at Larsen and Toubro Technological Services in the Oil & Gas domain, where Krishna implemented data analysis processes and optimized code using various tools and libraries. Krishna also has experience as a Full Stack Developer and Data Engineer at Quantum Mutual Fund in the Financial domain and Smart Solutions in the CRM and FMCG domains.\n",
      "\n",
      "Krishna has a strong background in technical skills, including programming languages, data engineering & analysis, databases, and expertise in DSA, algorithm analysis, and distributed systems. Krishna holds a Master of Technology in Computer Engineering with a specialization in Software Engineering from Veermata Jijabai Technological Institute (VJTI), Mumbai, and a Bachelor of Engineering in Computer Engineering from the University of Mumbai. Krishna also has recent certifications from LinkedIn and Pluralsight related to Python and data wrangling.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Retrieve relevant documents for: In which company Krishna works?',\n",
       " 'output': \"Based on the information provided by the tool, Krishna Dipayan Bhunia works as a Senior Data Engineer at Capgemini, serving the client Procter & Gamble (P&G). Krishna's work involves data engineering pipelines using Python, Pandas, NumPy, MS-SQL 2022, Azure, and Docker. One of Krishna's key achievements at Capgemini includes reducing defects and leakages in product manufacturing by half by efficiently contextualizing data from multiple sources.\\n\\nPreviously, Krishna worked as a Software Engineer and Data Analyst at Larsen and Toubro Technological Services in the Oil & Gas domain, where Krishna implemented data analysis processes and optimized code using various tools and libraries. Krishna also has experience as a Full Stack Developer and Data Engineer at Quantum Mutual Fund in the Financial domain and Smart Solutions in the CRM and FMCG domains.\\n\\nKrishna has a strong background in technical skills, including programming languages, data engineering & analysis, databases, and expertise in DSA, algorithm analysis, and distributed systems. Krishna holds a Master of Technology in Computer Engineering with a specialization in Software Engineering from Veermata Jijabai Technological Institute (VJTI), Mumbai, and a Bachelor of Engineering in Computer Engineering from the University of Mumbai. Krishna also has recent certifications from LinkedIn and Pluralsight related to Python and data wrangling.\"}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools = [retrieve_documents]\n",
    "query = \"In which company Krishna works?\"\n",
    "\n",
    "agent = create_tool_calling_agent(llm, tools, prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "agent_executor.invoke({\"input\": f\"Retrieve relevant documents for: {query}\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `retrieve_documents` with `{'query': 'where Krishna works'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mKRISHNA DIPAYAN BHUNIA \n",
      "Senior Software Engineer | Senior Data Engineer | Python Developer \n",
      "+91 865 200 7894 \n",
      " \n",
      "Github Link \n",
      "LinkedIn \n",
      " \n",
      "kri.career@gmail.com \n",
      "SUMMARY \n",
      "● Seasoned Software Engineer/Full Stack Developer with 12+ years of experience across industries like CRM, Oil & Gas, Financial, and FMCG. Over \n",
      "6 years of expertise in Python as a Data Engineer/Scientist, specializing in data engineering, analysis, and visualization.  \n",
      "● Proficient in writing clean, production-level Python code, with strong skills in Object-Oriented Programming, DSA, and relational databases. Used \n",
      "Python libraries such as NumPy, and Pandas for advanced data analysis and engineering stuff. \n",
      "● Experienced in designing software solutions using Python, C#, and back-end technologies. Adept in cloud platforms (AWS, Azure), containerization \n",
      "(Docker), and database management (SQL, No-SQL), with a strong ability to analyze algorithms and collaborate effectively. \n",
      "EDUCATION \n",
      "● M.TECH (Computer Eng. with Specialization in Software Eng.)   \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Aug 2016 – Jun 2018 \n",
      "Veermata Jijabai Technological Institute (VJTI), Mumbai \n",
      "● B.E Computer Engineering (University of Mumbai) \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Aug 2008 – Jun 2012 \n",
      "TECHNICAL SKILLS \n",
      "● Languages & Frameworks: Python, NumPy, Pandas, Java, JavaScript (jQuery), MS SQL, PL-SQL, Oracle, C# (C-Sharp), Docker, Github. \n",
      "● Data Engineering & Analysis: Data Science and Data engineering processes, Data Analysis, Classification, Regression Analysis, Cluster Analysis, \n",
      "Data Visualization, Statistics, AWS (Amazon Web Services), Azure. \n",
      "● Databases: MS SQL Server (2019/2017), MySQL, MongoDB, Oracle 19c. \n",
      "● Expertise: DSA (Data Structures & Algorithms), Algorithm Analysis (Space & Time Complexity), Distributed Systems & Architecture. \n",
      "WORK EXPERIENCE & PROJECTS \n",
      "Senior Data Engineer in Capgemini \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "    Oct 2021 – Present \n",
      "Client - P&G(Procter & Gamble) - (Role - Data Engineer, Developer)  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "            \n",
      "Pune(Remote) \n",
      "● Technical Stack: Python, Pandas, NumPy, MS-SQL 2022, JSON, VS Code, Azure, Azure Pipelines, Docker, GitHub. \n",
      "● Proficient in Python for data engineering pipelines. \n",
      "● Experienced with Pandas, NumPy for data processing and analysis. \n",
      "● Skilled in client communication to gather data and project requirements. \n",
      "● Set up and configured Azure pipelines for CI/CD of data pipelines. \n",
      "● Created and optimized MS-SQL tables for efficient data storage. \n",
      "● Populated metrics to Grafana Dashboard from manufacturing data. \n",
      "● Designed ETL pipelines for data extraction from various sources. \n",
      "● Applied statistical techniques and optimization for data contextualization. \n",
      "● Delivered pipelines for leakage, defect detection, and insights. \n",
      "Key Achievement - With accommodating data from multiple sources makes it easy to contextualize data for easy analysis purposes, which improves \n",
      "the manufacturing lines are more adhered to not making defects/leakages in product manufacturing. The defects/leakages have been reduced to half \n",
      "of the previous defects identified. \n",
      "Software Engineer and Data Analyst in Larsen and Toubro Technological Services worked in Oil & Gas Domain \n",
      " \n",
      "Jan 2020 – Oct 2021 \n",
      " (Role – Data Engineer, Developer) \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Vadodara \n",
      "  (Python, NumPy, Pandas, SciPy, MS SQL Server 2017, Oracle 11G and Oracle 10G, xlrd, xlwt, openpyxl (Anaconda Navigator Framework)). \n",
      "Plant Portal Data Analysis [ Covestro, IPEP (Data analytics team)] \n",
      "● Implementation for the process of Analysis (Deduplication, Data Seagration, File Reference Missing in Smart Plant portal, finding different \n",
      "equipment properties in plant portal, Different levels of data missing. Different file formats, Segregation and separation of different language \n",
      "formats, metadata, and migrated documents in smart plant portal).  \n",
      "● Used Task, Thread and Parallel process and optimized codes with Singleton and Factory Design pattern. Excel dynamic read and write with \n",
      "formatting using closedXML, OpenXML, EPPlus, Excel Data Reader. \n",
      "● Key Achievement – Computation has reduced time significantly for a large dataset and customer delivery satisfaction was achieved on-time. \n",
      "SPI (Smart Plant Integration) and SPEC [ Covestro, IPEP (Data analytics team)] \n",
      "● Data Comparison of migrating data from oracle to SQL Server. Generation of Excel Data and its comparison using EPPlus and generating reports. \n",
      "Created Custom controls and User controls, Delegation of custom and user controls, Background task and multithreading with parameterized \n",
      "arguments, async and await tasks, a comparison using multi and different primary keys also with dynamic and different key columns. Data view \n",
      "sync. Comparison checks for BLOB data and other Hash File data. \n",
      "● Implemented Task, Thread and Parallel process and optimized codes with Singleton and Factory Design pattern. Excel dynamic read and write \n",
      "with formatting using closedXML, EPPlus, ExcelDataReader and Dataset in Visual Studio. Logging facility in python and used data frames. \n",
      "● Key Achievement – With multithreading and parallel processing output time has been significantly improved by more than 90%. Worked closely \n",
      "with cross-functional teams including plant engineers, data owners and site engineers to design, build and deliver end-to end solutions to their \n",
      "individual data conversion and wrangling problems. \n",
      "Software Engineer in Quantum Mutual Fund worked in Financial Domain \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " Aug 2018 - Jan 2020 \n",
      "(Role – Full Stack Developer/ Data Engineer)  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "     Mumba \n",
      "Invest Online (INON) - Backend Developer \n",
      "● Developed business logic for investment in Q-MF & transaction, Re-Purchase, Redemption, Switch, New Purchase, STP, SWP, SIP. \n",
      "● Optimized backend SQL code for Investors, used to do KYC according to their citizenship with different criteria & process form in different queues. \n",
      "Software Engineer in Smart Solutions worked in CRM, FMCG  Domain \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Oct 2013 - July 2018 \n",
      "(Role – Full Stack Developer)  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "     Mumbai \n",
      "Inventory management and services - Backend Developer  \n",
      "● Capturing, managing, and maintaining inventories and managing. Assigning blocks and holding inventories. \n",
      "Graduate Engineer Trainee in Godrej & Boyce worked in CRM Domain \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Oct 2012 - Sept 2013 \n",
      "(Role – Full Stack Developer)  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "     Mumbai \n",
      "Timespan- Attendance and Leave management system, Assets management system & Drawer depository safe - Application  \n",
      "● Capturing, managing, and maintaining attendance and leaves for the employee in the company. Migrating assets and system information to \n",
      "standard format. \n",
      "CERTIFICATION AND LINKS (Recent) \n",
      "● LinkedIn & Pluralsight Certificates - #Using Python with Excel  #Data Wrangling with Pandas for Data Engineers\u001b[0m\u001b[32;1m\u001b[1;3mKrishna Dipayan Bhunia is a Senior Software Engineer and Data Engineer with 12+ years of experience across various industries. He specializes in Python as a Data Engineer/Scientist and has expertise in data engineering, analysis, and visualization. Krishna has worked with companies like Capgemini, Larsen and Toubro Technological Services, Quantum Mutual Fund, and Godrej & Boyce. He is proficient in various programming languages, data engineering & analysis tools, databases, and technical skills such as Python, NumPy, Pandas, Java, JavaScript, MS SQL, PL-SQL, Oracle, C#, Docker, and Github. Krishna has a M.Tech in Computer Engineering with Specialization in Software Eng. from Veermata Jijabai Technological Institute (VJTI), Mumbai and a B.E in Computer Engineering from the University of Mumbai. He can be reached at kri.career@gmail.com.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Retrieve relevant documents for: Pls answer in one word, where Krishna works?',\n",
       " 'output': 'Krishna Dipayan Bhunia is a Senior Software Engineer and Data Engineer with 12+ years of experience across various industries. He specializes in Python as a Data Engineer/Scientist and has expertise in data engineering, analysis, and visualization. Krishna has worked with companies like Capgemini, Larsen and Toubro Technological Services, Quantum Mutual Fund, and Godrej & Boyce. He is proficient in various programming languages, data engineering & analysis tools, databases, and technical skills such as Python, NumPy, Pandas, Java, JavaScript, MS SQL, PL-SQL, Oracle, C#, Docker, and Github. Krishna has a M.Tech in Computer Engineering with Specialization in Software Eng. from Veermata Jijabai Technological Institute (VJTI), Mumbai and a B.E in Computer Engineering from the University of Mumbai. He can be reached at kri.career@gmail.com.'}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools = [retrieve_documents]\n",
    "query = \"Pls answer in one word, where Krishna works?\"\n",
    "\n",
    "agent = create_tool_calling_agent(llm, tools, prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "agent_executor.invoke({\"input\": f\"Retrieve relevant documents for: {query}\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `retrieve_documents` with `{'query': 'Krishna 2012 work location'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mKRISHNA DIPAYAN BHUNIA \n",
      "Senior Software Engineer | Senior Data Engineer | Python Developer \n",
      "+91 865 200 7894 \n",
      " \n",
      "Github Link \n",
      "LinkedIn \n",
      " \n",
      "kri.career@gmail.com \n",
      "SUMMARY \n",
      "● Seasoned Software Engineer/Full Stack Developer with 12+ years of experience across industries like CRM, Oil & Gas, Financial, and FMCG. Over \n",
      "6 years of expertise in Python as a Data Engineer/Scientist, specializing in data engineering, analysis, and visualization.  \n",
      "● Proficient in writing clean, production-level Python code, with strong skills in Object-Oriented Programming, DSA, and relational databases. Used \n",
      "Python libraries such as NumPy, and Pandas for advanced data analysis and engineering stuff. \n",
      "● Experienced in designing software solutions using Python, C#, and back-end technologies. Adept in cloud platforms (AWS, Azure), containerization \n",
      "(Docker), and database management (SQL, No-SQL), with a strong ability to analyze algorithms and collaborate effectively. \n",
      "EDUCATION \n",
      "● M.TECH (Computer Eng. with Specialization in Software Eng.)   \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Aug 2016 – Jun 2018 \n",
      "Veermata Jijabai Technological Institute (VJTI), Mumbai \n",
      "● B.E Computer Engineering (University of Mumbai) \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Aug 2008 – Jun 2012 \n",
      "TECHNICAL SKILLS \n",
      "● Languages & Frameworks: Python, NumPy, Pandas, Java, JavaScript (jQuery), MS SQL, PL-SQL, Oracle, C# (C-Sharp), Docker, Github. \n",
      "● Data Engineering & Analysis: Data Science and Data engineering processes, Data Analysis, Classification, Regression Analysis, Cluster Analysis, \n",
      "Data Visualization, Statistics, AWS (Amazon Web Services), Azure. \n",
      "● Databases: MS SQL Server (2019/2017), MySQL, MongoDB, Oracle 19c. \n",
      "● Expertise: DSA (Data Structures & Algorithms), Algorithm Analysis (Space & Time Complexity), Distributed Systems & Architecture. \n",
      "WORK EXPERIENCE & PROJECTS \n",
      "Senior Data Engineer in Capgemini \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "    Oct 2021 – Present \n",
      "Client - P&G(Procter & Gamble) - (Role - Data Engineer, Developer)  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "            \n",
      "Pune(Remote) \n",
      "● Technical Stack: Python, Pandas, NumPy, MS-SQL 2022, JSON, VS Code, Azure, Azure Pipelines, Docker, GitHub. \n",
      "● Proficient in Python for data engineering pipelines. \n",
      "● Experienced with Pandas, NumPy for data processing and analysis. \n",
      "● Skilled in client communication to gather data and project requirements. \n",
      "● Set up and configured Azure pipelines for CI/CD of data pipelines. \n",
      "● Created and optimized MS-SQL tables for efficient data storage. \n",
      "● Populated metrics to Grafana Dashboard from manufacturing data. \n",
      "● Designed ETL pipelines for data extraction from various sources. \n",
      "● Applied statistical techniques and optimization for data contextualization. \n",
      "● Delivered pipelines for leakage, defect detection, and insights. \n",
      "Key Achievement - With accommodating data from multiple sources makes it easy to contextualize data for easy analysis purposes, which improves \n",
      "the manufacturing lines are more adhered to not making defects/leakages in product manufacturing. The defects/leakages have been reduced to half \n",
      "of the previous defects identified. \n",
      "Software Engineer and Data Analyst in Larsen and Toubro Technological Services worked in Oil & Gas Domain \n",
      " \n",
      "Jan 2020 – Oct 2021 \n",
      " (Role – Data Engineer, Developer) \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Vadodara \n",
      "  (Python, NumPy, Pandas, SciPy, MS SQL Server 2017, Oracle 11G and Oracle 10G, xlrd, xlwt, openpyxl (Anaconda Navigator Framework)). \n",
      "Plant Portal Data Analysis [ Covestro, IPEP (Data analytics team)] \n",
      "● Implementation for the process of Analysis (Deduplication, Data Seagration, File Reference Missing in Smart Plant portal, finding different \n",
      "equipment properties in plant portal, Different levels of data missing. Different file formats, Segregation and separation of different language \n",
      "formats, metadata, and migrated documents in smart plant portal).  \n",
      "● Used Task, Thread and Parallel process and optimized codes with Singleton and Factory Design pattern. Excel dynamic read and write with \n",
      "formatting using closedXML, OpenXML, EPPlus, Excel Data Reader. \n",
      "● Key Achievement – Computation has reduced time significantly for a large dataset and customer delivery satisfaction was achieved on-time. \n",
      "SPI (Smart Plant Integration) and SPEC [ Covestro, IPEP (Data analytics team)] \n",
      "● Data Comparison of migrating data from oracle to SQL Server. Generation of Excel Data and its comparison using EPPlus and generating reports. \n",
      "Created Custom controls and User controls, Delegation of custom and user controls, Background task and multithreading with parameterized \n",
      "arguments, async and await tasks, a comparison using multi and different primary keys also with dynamic and different key columns. Data view \n",
      "sync. Comparison checks for BLOB data and other Hash File data. \n",
      "● Implemented Task, Thread and Parallel process and optimized codes with Singleton and Factory Design pattern. Excel dynamic read and write \n",
      "with formatting using closedXML, EPPlus, ExcelDataReader and Dataset in Visual Studio. Logging facility in python and used data frames. \n",
      "● Key Achievement – With multithreading and parallel processing output time has been significantly improved by more than 90%. Worked closely \n",
      "with cross-functional teams including plant engineers, data owners and site engineers to design, build and deliver end-to end solutions to their \n",
      "individual data conversion and wrangling problems. \n",
      "Software Engineer in Quantum Mutual Fund worked in Financial Domain \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " Aug 2018 - Jan 2020 \n",
      "(Role – Full Stack Developer/ Data Engineer)  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "     Mumba \n",
      "Invest Online (INON) - Backend Developer \n",
      "● Developed business logic for investment in Q-MF & transaction, Re-Purchase, Redemption, Switch, New Purchase, STP, SWP, SIP. \n",
      "● Optimized backend SQL code for Investors, used to do KYC according to their citizenship with different criteria & process form in different queues. \n",
      "Software Engineer in Smart Solutions worked in CRM, FMCG  Domain \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Oct 2013 - July 2018 \n",
      "(Role – Full Stack Developer)  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "     Mumbai \n",
      "Inventory management and services - Backend Developer  \n",
      "● Capturing, managing, and maintaining inventories and managing. Assigning blocks and holding inventories. \n",
      "Graduate Engineer Trainee in Godrej & Boyce worked in CRM Domain \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Oct 2012 - Sept 2013 \n",
      "(Role – Full Stack Developer)  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "     Mumbai \n",
      "Timespan- Attendance and Leave management system, Assets management system & Drawer depository safe - Application  \n",
      "● Capturing, managing, and maintaining attendance and leaves for the employee in the company. Migrating assets and system information to \n",
      "standard format. \n",
      "CERTIFICATION AND LINKS (Recent) \n",
      "● LinkedIn & Pluralsight Certificates - #Using Python with Excel  #Data Wrangling with Pandas for Data Engineers\u001b[0m\u001b[32;1m\u001b[1;3mBased on the information provided by the tool, Krishna Dipayan Bhunia was a Senior Software Engineer and Data Engineer with over 12 years of experience in various industries like CRM, Oil & Gas, Financial, and FMCG. He specializes in data engineering, analysis, and visualization using Python libraries such as NumPy and Pandas.\n",
      "\n",
      "In 2012, Krishna was working as a Graduate Engineer Trainee in Godrej & Boyce, which is a company in the CRM domain.\n",
      "\n",
      "Here is the final answer:\n",
      "\n",
      "In 2012, Krishna Dipayan Bhunia was working as a Graduate Engineer Trainee in Godrej & Boyce, which is a company in the CRM domain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Retrieve relevant documents for: where Krishna was working on 2012',\n",
       " 'output': 'Based on the information provided by the tool, Krishna Dipayan Bhunia was a Senior Software Engineer and Data Engineer with over 12 years of experience in various industries like CRM, Oil & Gas, Financial, and FMCG. He specializes in data engineering, analysis, and visualization using Python libraries such as NumPy and Pandas.\\n\\nIn 2012, Krishna was working as a Graduate Engineer Trainee in Godrej & Boyce, which is a company in the CRM domain.\\n\\nHere is the final answer:\\n\\nIn 2012, Krishna Dipayan Bhunia was working as a Graduate Engineer Trainee in Godrej & Boyce, which is a company in the CRM domain.'}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools = [retrieve_documents]\n",
    "query = \"where Krishna was working on 2012\"\n",
    "\n",
    "agent = create_tool_calling_agent(llm, tools, prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "agent_executor.invoke({\"input\": f\"Retrieve relevant documents for: {query}\"})\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_kri_mes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
