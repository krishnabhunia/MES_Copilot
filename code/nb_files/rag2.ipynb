{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain_groq in /home/kb/MES_Copilot/POC/venv_kri_mes/lib/python3.10/site-packages (0.2.0)\n",
      "Requirement already satisfied: groq<1,>=0.4.1 in /home/kb/MES_Copilot/POC/venv_kri_mes/lib/python3.10/site-packages (from langchain_groq) (0.11.0)\n",
      "Requirement already satisfied: langchain-core<0.4,>=0.3 in /home/kb/MES_Copilot/POC/venv_kri_mes/lib/python3.10/site-packages (from langchain_groq) (0.3.9)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /home/kb/MES_Copilot/POC/venv_kri_mes/lib/python3.10/site-packages (from groq<1,>=0.4.1->langchain_groq) (4.6.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /home/kb/MES_Copilot/POC/venv_kri_mes/lib/python3.10/site-packages (from groq<1,>=0.4.1->langchain_groq) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/kb/MES_Copilot/POC/venv_kri_mes/lib/python3.10/site-packages (from groq<1,>=0.4.1->langchain_groq) (0.27.2)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /home/kb/MES_Copilot/POC/venv_kri_mes/lib/python3.10/site-packages (from groq<1,>=0.4.1->langchain_groq) (2.9.2)\n",
      "Requirement already satisfied: sniffio in /home/kb/MES_Copilot/POC/venv_kri_mes/lib/python3.10/site-packages (from groq<1,>=0.4.1->langchain_groq) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /home/kb/MES_Copilot/POC/venv_kri_mes/lib/python3.10/site-packages (from groq<1,>=0.4.1->langchain_groq) (4.12.2)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /home/kb/MES_Copilot/POC/venv_kri_mes/lib/python3.10/site-packages (from langchain-core<0.4,>=0.3->langchain_groq) (6.0.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /home/kb/MES_Copilot/POC/venv_kri_mes/lib/python3.10/site-packages (from langchain-core<0.4,>=0.3->langchain_groq) (1.33)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in /home/kb/MES_Copilot/POC/venv_kri_mes/lib/python3.10/site-packages (from langchain-core<0.4,>=0.3->langchain_groq) (0.1.132)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /home/kb/MES_Copilot/POC/venv_kri_mes/lib/python3.10/site-packages (from langchain-core<0.4,>=0.3->langchain_groq) (24.1)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /home/kb/MES_Copilot/POC/venv_kri_mes/lib/python3.10/site-packages (from langchain-core<0.4,>=0.3->langchain_groq) (8.5.0)\n",
      "Requirement already satisfied: idna>=2.8 in /home/kb/MES_Copilot/POC/venv_kri_mes/lib/python3.10/site-packages (from anyio<5,>=3.5.0->groq<1,>=0.4.1->langchain_groq) (3.10)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/kb/MES_Copilot/POC/venv_kri_mes/lib/python3.10/site-packages (from anyio<5,>=3.5.0->groq<1,>=0.4.1->langchain_groq) (1.2.2)\n",
      "Requirement already satisfied: certifi in /home/kb/MES_Copilot/POC/venv_kri_mes/lib/python3.10/site-packages (from httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain_groq) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /home/kb/MES_Copilot/POC/venv_kri_mes/lib/python3.10/site-packages (from httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain_groq) (1.0.6)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/kb/MES_Copilot/POC/venv_kri_mes/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain_groq) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /home/kb/MES_Copilot/POC/venv_kri_mes/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4,>=0.3->langchain_groq) (3.0.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /home/kb/MES_Copilot/POC/venv_kri_mes/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3->langchain_groq) (3.10.7)\n",
      "Requirement already satisfied: requests<3,>=2 in /home/kb/MES_Copilot/POC/venv_kri_mes/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3->langchain_groq) (2.32.3)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /home/kb/MES_Copilot/POC/venv_kri_mes/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3->langchain_groq) (1.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/kb/MES_Copilot/POC/venv_kri_mes/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->groq<1,>=0.4.1->langchain_groq) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /home/kb/MES_Copilot/POC/venv_kri_mes/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->groq<1,>=0.4.1->langchain_groq) (2.23.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/kb/MES_Copilot/POC/venv_kri_mes/lib/python3.10/site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3->langchain_groq) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/kb/MES_Copilot/POC/venv_kri_mes/lib/python3.10/site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3->langchain_groq) (2.2.3)\n"
     ]
    }
   ],
   "source": [
    "! pip install langchain_groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "from langchain.agents import create_tool_calling_agent\n",
    "from langchain.agents import AgentExecutor\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kb/MES_Copilot/POC/venv_kri_mes/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "# Load documents\n",
    "loader = TextLoader(\"krishna_doc.txt\")\n",
    "documents = loader.load()\n",
    "\n",
    "# Split text into chunks\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "embedding_model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "embeddings = HuggingFaceEmbeddings(model_name=embedding_model_name)\n",
    "\n",
    "vector_store = FAISS.from_documents(texts, embeddings)\n",
    "retriever = vector_store.as_retriever()\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant\"),\n",
    "        (\"placeholder\", \"{chat_history}\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "        (\"placeholder\", \"{agent_scratchpad}\")\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGroq(\n",
    "    model=\"mixtral-8x7b-32768\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2\n",
    ")\n",
    "\n",
    "# llm = ChatGroq(model=\"llama3-70b-8192\", temperature=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def retrieve_documents(query: str) -> str:\n",
    "    \"\"\"Retrieve relevant documents based on the query\"\"\"\n",
    "    docs = retriever.invoke(query)\n",
    "    return \"\\n\".join([doc.page_content for doc in docs])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `retrieve_documents` with `{'query': 'who is Krishna?'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mKRISHNA DIPAYAN BHUNIA Senior Software Engineer | Senior Data Engineer | Python Developer+91 865 200 7894Github LinkLinkedInkri.career@gmail.comSUMMARY●Seasoned Software Engineer/Full Stack Developer with 12+ years of experienceacross industries like CRM, Oil & Gas, Financial, and FMCG. Over 6 years of expertise in Python as a Data Engineer/Scientist, specializing in data engineering, analysis, and visualization. ●Proficient in writing clean, production-level Python code, with strong skills in Object-Oriented Programming, DSA, and relational databases. Used Python libraries such as NumPy, and Pandasfor advanced data analysis and engineering stuff.●Experienced in designing software solutions using Python, C#,and back-end technologies. Adept in cloud platforms (AWS, Azure), containerization (Docker), and database management (SQL, No-SQL), with a strong ability to analyze algorithms and collaborate effectively.EDUCATION●M.TECH (Computer Eng. with Specializationin Software Eng.)Aug 2016 –Jun 2018Veermata Jijabai Technological Institute (VJTI), Mumbai●B.E Computer Engineering (University of Mumbai)Aug 2008 –Jun 2012TECHNICAL SKILLS●Languages & Frameworks: Python, NumPy, Pandas, Java, JavaScript (jQuery), MS SQL, PL-SQL, Oracle, C# (C-Sharp), Docker, Github.●Data Engineering & Analysis:Data Science and Data engineering processes, Data Analysis, Classification, Regression Analysis, Cluster Analysis, Data Visualization, Statistics,AWS (Amazon Web Services), Azure.●Databases: MS SQL Server (2019/2017), MySQL, MongoDB, Oracle 19c.●Expertise: DSA (Data Structures & Algorithms), Algorithm Analysis (Space & Time Complexity), Distributed Systems & Architecture.WORK EXPERIENCE & PROJECTSSenior Data Engineer in CapgeminiOct 2021 –PresentClient -P&G(Procter & Gamble) -(Role -Data Engineer, Developer)Pune(Remote)●Technical Stack: Python, Pandas, NumPy, MS-SQL 2022, JSON, VS Code, Azure, Azure Pipelines, Docker, GitHub.●Proficient in Python for data engineering pipelines.●Experienced with Pandas, NumPy for data processing and analysis.●Skilled in client communication to gather data and project requirements.●Set up and configured Azure pipelines for CI/CD of data pipelines.●Created and optimized MS-SQL tables for efficient data storage.●Populated metrics to Grafana Dashboard from manufacturing data.●Designed ETL pipelines for data extraction from various sources.●Applied statistical techniques and optimization for data contextualization.●Delivered pipelines for leakage, defect detection, and insights.Key Achievement -With accommodating data from multiple sources makes it easy to contextualize data for easy analysis purposes, which improves the manufacturing lines are more adhered to not making defects/leakages in product manufacturing. The defects/leakages have been reduced to half of the previous defects identified.Software Engineer and Data Analystin Larsen and Toubro Technological Servicesworked in Oil & GasDomainJan 2020 –Oct 2021(Role –Data Engineer, Developer)Vadodara(Python, NumPy, Pandas, SciPy, MS SQL Server 2017, Oracle 11Gand Oracle 10G, xlrd, xlwt, openpyxl (Anaconda Navigator Framework)).Plant Portal Data Analysis [ Covestro, IPEP (Data analytics team)]●Implementation  for  the  process  of  Analysis  (Deduplication,  Data  Seagration,  File  Reference  Missing  in Smart  Plantportal,  finding  different equipment properties in plant portal, Different levels of data missing. Different file formats, Segregationand separation of different language formats, metadata, and migrated documents in smart plant portal). ●Used Task, Thread and Parallel process and optimizedcodes with Singleton and Factory Design pattern.Excel dynamic read and write with formattingusing closedXML, OpenXML, EPPlus, Excel Data Reader.●Key Achievement –Computation has reduced time significantly for a large dataset and customer delivery satisfaction was achieved on-time.SPI (Smart PlantIntegration) and SPEC [ Covestro, IPEP (Data analytics team)]●Data Comparison of migrating data from oracle to SQL Server. Generation of Excel Data and its comparison using EPPlus and generating reports. Created Custom controls and User controls, Delegation of custom and user controls, Background task and multithreading with parameterized arguments, async and await tasks, a comparison using multi and different primary keys also with dynamic and different key columns. Data view sync. Comparison checks for BLOB data and other Hash File data.●Implemented Task, Thread and Parallel process and optimizedcodes with Singleton and Factory Design pattern.Excel dynamic read and write with formattingusing closedXML, EPPlus, ExcelDataReader and Dataset in Visual Studio. Logging facility in python and used data frames.●Key Achievement –With multithreading and parallel processing output time has been significantly improved by more than 90%. Worked closely with cross-functional teams including plant engineers, data owners and site engineers to design, build and deliver end-to end solutions to their individual data conversion and wrangling problems.Software Engineerin Quantum Mutual Fundworked in FinancialDomainAug 2018 -Jan 2020(Role –Full Stack Developer/ Data Engineer)MumbaInvest Online (INON) -Backend Developer●Developed business logic for investment in Q-MF & transaction, Re-Purchase, Redemption, Switch, New Purchase, STP, SWP, SIP.●Optimized backend SQL code for Investors, usedto do KYC according to their citizenship with different criteria & process form in different queues.Software Engineer in Smart Solutionsworked in CRM, FMCG DomainOct 2013 -July 2018(Role –Full Stack Developer)MumbaiInventory management and services -Backend Developer ●Capturing, managing, and maintaining inventories and managing. Assigningblocks and holdinginventories.Graduate Engineer Trainee in Godrej & Boyce worked in CRM DomainOct 2012 -Sept 2013(Role –Full Stack Developer)MumbaiTimespan-Attendance and Leave management system, Assets management system & Drawer depository safe -Application ●Capturing, managing, and maintaining attendance and leaves for the employee in the company. Migrating assets and system information to standard format.CERTIFICATION AND LINKS(Recent)●LinkedIn & Pluralsight Certificates -#Using Python with Excel#Data Wrangling with Pandas for DataEngineers\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `retrieve_documents` with `{'query': 'Krishna'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mKRISHNA DIPAYAN BHUNIA Senior Software Engineer | Senior Data Engineer | Python Developer+91 865 200 7894Github LinkLinkedInkri.career@gmail.comSUMMARY●Seasoned Software Engineer/Full Stack Developer with 12+ years of experienceacross industries like CRM, Oil & Gas, Financial, and FMCG. Over 6 years of expertise in Python as a Data Engineer/Scientist, specializing in data engineering, analysis, and visualization. ●Proficient in writing clean, production-level Python code, with strong skills in Object-Oriented Programming, DSA, and relational databases. Used Python libraries such as NumPy, and Pandasfor advanced data analysis and engineering stuff.●Experienced in designing software solutions using Python, C#,and back-end technologies. Adept in cloud platforms (AWS, Azure), containerization (Docker), and database management (SQL, No-SQL), with a strong ability to analyze algorithms and collaborate effectively.EDUCATION●M.TECH (Computer Eng. with Specializationin Software Eng.)Aug 2016 –Jun 2018Veermata Jijabai Technological Institute (VJTI), Mumbai●B.E Computer Engineering (University of Mumbai)Aug 2008 –Jun 2012TECHNICAL SKILLS●Languages & Frameworks: Python, NumPy, Pandas, Java, JavaScript (jQuery), MS SQL, PL-SQL, Oracle, C# (C-Sharp), Docker, Github.●Data Engineering & Analysis:Data Science and Data engineering processes, Data Analysis, Classification, Regression Analysis, Cluster Analysis, Data Visualization, Statistics,AWS (Amazon Web Services), Azure.●Databases: MS SQL Server (2019/2017), MySQL, MongoDB, Oracle 19c.●Expertise: DSA (Data Structures & Algorithms), Algorithm Analysis (Space & Time Complexity), Distributed Systems & Architecture.WORK EXPERIENCE & PROJECTSSenior Data Engineer in CapgeminiOct 2021 –PresentClient -P&G(Procter & Gamble) -(Role -Data Engineer, Developer)Pune(Remote)●Technical Stack: Python, Pandas, NumPy, MS-SQL 2022, JSON, VS Code, Azure, Azure Pipelines, Docker, GitHub.●Proficient in Python for data engineering pipelines.●Experienced with Pandas, NumPy for data processing and analysis.●Skilled in client communication to gather data and project requirements.●Set up and configured Azure pipelines for CI/CD of data pipelines.●Created and optimized MS-SQL tables for efficient data storage.●Populated metrics to Grafana Dashboard from manufacturing data.●Designed ETL pipelines for data extraction from various sources.●Applied statistical techniques and optimization for data contextualization.●Delivered pipelines for leakage, defect detection, and insights.Key Achievement -With accommodating data from multiple sources makes it easy to contextualize data for easy analysis purposes, which improves the manufacturing lines are more adhered to not making defects/leakages in product manufacturing. The defects/leakages have been reduced to half of the previous defects identified.Software Engineer and Data Analystin Larsen and Toubro Technological Servicesworked in Oil & GasDomainJan 2020 –Oct 2021(Role –Data Engineer, Developer)Vadodara(Python, NumPy, Pandas, SciPy, MS SQL Server 2017, Oracle 11Gand Oracle 10G, xlrd, xlwt, openpyxl (Anaconda Navigator Framework)).Plant Portal Data Analysis [ Covestro, IPEP (Data analytics team)]●Implementation  for  the  process  of  Analysis  (Deduplication,  Data  Seagration,  File  Reference  Missing  in Smart  Plantportal,  finding  different equipment properties in plant portal, Different levels of data missing. Different file formats, Segregationand separation of different language formats, metadata, and migrated documents in smart plant portal). ●Used Task, Thread and Parallel process and optimizedcodes with Singleton and Factory Design pattern.Excel dynamic read and write with formattingusing closedXML, OpenXML, EPPlus, Excel Data Reader.●Key Achievement –Computation has reduced time significantly for a large dataset and customer delivery satisfaction was achieved on-time.SPI (Smart PlantIntegration) and SPEC [ Covestro, IPEP (Data analytics team)]●Data Comparison of migrating data from oracle to SQL Server. Generation of Excel Data and its comparison using EPPlus and generating reports. Created Custom controls and User controls, Delegation of custom and user controls, Background task and multithreading with parameterized arguments, async and await tasks, a comparison using multi and different primary keys also with dynamic and different key columns. Data view sync. Comparison checks for BLOB data and other Hash File data.●Implemented Task, Thread and Parallel process and optimizedcodes with Singleton and Factory Design pattern.Excel dynamic read and write with formattingusing closedXML, EPPlus, ExcelDataReader and Dataset in Visual Studio. Logging facility in python and used data frames.●Key Achievement –With multithreading and parallel processing output time has been significantly improved by more than 90%. Worked closely with cross-functional teams including plant engineers, data owners and site engineers to design, build and deliver end-to end solutions to their individual data conversion and wrangling problems.Software Engineerin Quantum Mutual Fundworked in FinancialDomainAug 2018 -Jan 2020(Role –Full Stack Developer/ Data Engineer)MumbaInvest Online (INON) -Backend Developer●Developed business logic for investment in Q-MF & transaction, Re-Purchase, Redemption, Switch, New Purchase, STP, SWP, SIP.●Optimized backend SQL code for Investors, usedto do KYC according to their citizenship with different criteria & process form in different queues.Software Engineer in Smart Solutionsworked in CRM, FMCG DomainOct 2013 -July 2018(Role –Full Stack Developer)MumbaiInventory management and services -Backend Developer ●Capturing, managing, and maintaining inventories and managing. Assigningblocks and holdinginventories.Graduate Engineer Trainee in Godrej & Boyce worked in CRM DomainOct 2012 -Sept 2013(Role –Full Stack Developer)MumbaiTimespan-Attendance and Leave management system, Assets management system & Drawer depository safe -Application ●Capturing, managing, and maintaining attendance and leaves for the employee in the company. Migrating assets and system information to standard format.CERTIFICATION AND LINKS(Recent)●LinkedIn & Pluralsight Certificates -#Using Python with Excel#Data Wrangling with Pandas for DataEngineers\u001b[0m\u001b[32;1m\u001b[1;3mThe text you provided appears to be a resume or a professional profile of someone named Krishna Dipayan Bhunia, who is a software engineer and data engineer with expertise in Python, data analysis, and visualization. However, this does not seem to be related to the Hindu deity Krishna. If you are looking for information on the Hindu deity Krishna, I'd be happy to provide that. Krishna is a major deity in Hinduism and is considered the eighth avatar, or incarnation, of the god Vishnu. He is known for his wisdom, courage, and divine power.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Retrieve relevant documents for: who is Krishna?',\n",
       " 'output': \"The text you provided appears to be a resume or a professional profile of someone named Krishna Dipayan Bhunia, who is a software engineer and data engineer with expertise in Python, data analysis, and visualization. However, this does not seem to be related to the Hindu deity Krishna. If you are looking for information on the Hindu deity Krishna, I'd be happy to provide that. Krishna is a major deity in Hinduism and is considered the eighth avatar, or incarnation, of the god Vishnu. He is known for his wisdom, courage, and divine power.\"}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools = [retrieve_documents]\n",
    "query = \"who is Krishna?\"\n",
    "\n",
    "agent = create_tool_calling_agent(llm, tools, prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "agent_executor.invoke({\"input\": f\"Retrieve relevant documents for: {query}\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `retrieve_documents` with `{'query': 'Krishna works'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mKRISHNA DIPAYAN BHUNIA Senior Software Engineer | Senior Data Engineer | Python Developer+91 865 200 7894Github LinkLinkedInkri.career@gmail.comSUMMARY●Seasoned Software Engineer/Full Stack Developer with 12+ years of experienceacross industries like CRM, Oil & Gas, Financial, and FMCG. Over 6 years of expertise in Python as a Data Engineer/Scientist, specializing in data engineering, analysis, and visualization. ●Proficient in writing clean, production-level Python code, with strong skills in Object-Oriented Programming, DSA, and relational databases. Used Python libraries such as NumPy, and Pandasfor advanced data analysis and engineering stuff.●Experienced in designing software solutions using Python, C#,and back-end technologies. Adept in cloud platforms (AWS, Azure), containerization (Docker), and database management (SQL, No-SQL), with a strong ability to analyze algorithms and collaborate effectively.EDUCATION●M.TECH (Computer Eng. with Specializationin Software Eng.)Aug 2016 –Jun 2018Veermata Jijabai Technological Institute (VJTI), Mumbai●B.E Computer Engineering (University of Mumbai)Aug 2008 –Jun 2012TECHNICAL SKILLS●Languages & Frameworks: Python, NumPy, Pandas, Java, JavaScript (jQuery), MS SQL, PL-SQL, Oracle, C# (C-Sharp), Docker, Github.●Data Engineering & Analysis:Data Science and Data engineering processes, Data Analysis, Classification, Regression Analysis, Cluster Analysis, Data Visualization, Statistics,AWS (Amazon Web Services), Azure.●Databases: MS SQL Server (2019/2017), MySQL, MongoDB, Oracle 19c.●Expertise: DSA (Data Structures & Algorithms), Algorithm Analysis (Space & Time Complexity), Distributed Systems & Architecture.WORK EXPERIENCE & PROJECTSSenior Data Engineer in CapgeminiOct 2021 –PresentClient -P&G(Procter & Gamble) -(Role -Data Engineer, Developer)Pune(Remote)●Technical Stack: Python, Pandas, NumPy, MS-SQL 2022, JSON, VS Code, Azure, Azure Pipelines, Docker, GitHub.●Proficient in Python for data engineering pipelines.●Experienced with Pandas, NumPy for data processing and analysis.●Skilled in client communication to gather data and project requirements.●Set up and configured Azure pipelines for CI/CD of data pipelines.●Created and optimized MS-SQL tables for efficient data storage.●Populated metrics to Grafana Dashboard from manufacturing data.●Designed ETL pipelines for data extraction from various sources.●Applied statistical techniques and optimization for data contextualization.●Delivered pipelines for leakage, defect detection, and insights.Key Achievement -With accommodating data from multiple sources makes it easy to contextualize data for easy analysis purposes, which improves the manufacturing lines are more adhered to not making defects/leakages in product manufacturing. The defects/leakages have been reduced to half of the previous defects identified.Software Engineer and Data Analystin Larsen and Toubro Technological Servicesworked in Oil & GasDomainJan 2020 –Oct 2021(Role –Data Engineer, Developer)Vadodara(Python, NumPy, Pandas, SciPy, MS SQL Server 2017, Oracle 11Gand Oracle 10G, xlrd, xlwt, openpyxl (Anaconda Navigator Framework)).Plant Portal Data Analysis [ Covestro, IPEP (Data analytics team)]●Implementation  for  the  process  of  Analysis  (Deduplication,  Data  Seagration,  File  Reference  Missing  in Smart  Plantportal,  finding  different equipment properties in plant portal, Different levels of data missing. Different file formats, Segregationand separation of different language formats, metadata, and migrated documents in smart plant portal). ●Used Task, Thread and Parallel process and optimizedcodes with Singleton and Factory Design pattern.Excel dynamic read and write with formattingusing closedXML, OpenXML, EPPlus, Excel Data Reader.●Key Achievement –Computation has reduced time significantly for a large dataset and customer delivery satisfaction was achieved on-time.SPI (Smart PlantIntegration) and SPEC [ Covestro, IPEP (Data analytics team)]●Data Comparison of migrating data from oracle to SQL Server. Generation of Excel Data and its comparison using EPPlus and generating reports. Created Custom controls and User controls, Delegation of custom and user controls, Background task and multithreading with parameterized arguments, async and await tasks, a comparison using multi and different primary keys also with dynamic and different key columns. Data view sync. Comparison checks for BLOB data and other Hash File data.●Implemented Task, Thread and Parallel process and optimizedcodes with Singleton and Factory Design pattern.Excel dynamic read and write with formattingusing closedXML, EPPlus, ExcelDataReader and Dataset in Visual Studio. Logging facility in python and used data frames.●Key Achievement –With multithreading and parallel processing output time has been significantly improved by more than 90%. Worked closely with cross-functional teams including plant engineers, data owners and site engineers to design, build and deliver end-to end solutions to their individual data conversion and wrangling problems.Software Engineerin Quantum Mutual Fundworked in FinancialDomainAug 2018 -Jan 2020(Role –Full Stack Developer/ Data Engineer)MumbaInvest Online (INON) -Backend Developer●Developed business logic for investment in Q-MF & transaction, Re-Purchase, Redemption, Switch, New Purchase, STP, SWP, SIP.●Optimized backend SQL code for Investors, usedto do KYC according to their citizenship with different criteria & process form in different queues.Software Engineer in Smart Solutionsworked in CRM, FMCG DomainOct 2013 -July 2018(Role –Full Stack Developer)MumbaiInventory management and services -Backend Developer ●Capturing, managing, and maintaining inventories and managing. Assigningblocks and holdinginventories.Graduate Engineer Trainee in Godrej & Boyce worked in CRM DomainOct 2012 -Sept 2013(Role –Full Stack Developer)MumbaiTimespan-Attendance and Leave management system, Assets management system & Drawer depository safe -Application ●Capturing, managing, and maintaining attendance and leaves for the employee in the company. Migrating assets and system information to standard format.CERTIFICATION AND LINKS(Recent)●LinkedIn & Pluralsight Certificates -#Using Python with Excel#Data Wrangling with Pandas for DataEngineers\u001b[0m\u001b[32;1m\u001b[1;3mTool use failed: no tool can be called with name answer_query\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Retrieve relevant documents for: Pls answer in one word, where Krishna works?',\n",
       " 'output': 'Tool use failed: no tool can be called with name answer_query'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools = [retrieve_documents]\n",
    "query = \"Pls answer in one word, where Krishna works?\"\n",
    "\n",
    "agent = create_tool_calling_agent(llm, tools, prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "agent_executor.invoke({\"input\": f\"Retrieve relevant documents for: {query}\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `retrieve_documents` with `{'query': 'where Krishna was working on 2012'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mKRISHNA DIPAYAN BHUNIA Senior Software Engineer | Senior Data Engineer | Python Developer+91 865 200 7894Github LinkLinkedInkri.career@gmail.comSUMMARY●Seasoned Software Engineer/Full Stack Developer with 12+ years of experienceacross industries like CRM, Oil & Gas, Financial, and FMCG. Over 6 years of expertise in Python as a Data Engineer/Scientist, specializing in data engineering, analysis, and visualization. ●Proficient in writing clean, production-level Python code, with strong skills in Object-Oriented Programming, DSA, and relational databases. Used Python libraries such as NumPy, and Pandasfor advanced data analysis and engineering stuff.●Experienced in designing software solutions using Python, C#,and back-end technologies. Adept in cloud platforms (AWS, Azure), containerization (Docker), and database management (SQL, No-SQL), with a strong ability to analyze algorithms and collaborate effectively.EDUCATION●M.TECH (Computer Eng. with Specializationin Software Eng.)Aug 2016 –Jun 2018Veermata Jijabai Technological Institute (VJTI), Mumbai●B.E Computer Engineering (University of Mumbai)Aug 2008 –Jun 2012TECHNICAL SKILLS●Languages & Frameworks: Python, NumPy, Pandas, Java, JavaScript (jQuery), MS SQL, PL-SQL, Oracle, C# (C-Sharp), Docker, Github.●Data Engineering & Analysis:Data Science and Data engineering processes, Data Analysis, Classification, Regression Analysis, Cluster Analysis, Data Visualization, Statistics,AWS (Amazon Web Services), Azure.●Databases: MS SQL Server (2019/2017), MySQL, MongoDB, Oracle 19c.●Expertise: DSA (Data Structures & Algorithms), Algorithm Analysis (Space & Time Complexity), Distributed Systems & Architecture.WORK EXPERIENCE & PROJECTSSenior Data Engineer in CapgeminiOct 2021 –PresentClient -P&G(Procter & Gamble) -(Role -Data Engineer, Developer)Pune(Remote)●Technical Stack: Python, Pandas, NumPy, MS-SQL 2022, JSON, VS Code, Azure, Azure Pipelines, Docker, GitHub.●Proficient in Python for data engineering pipelines.●Experienced with Pandas, NumPy for data processing and analysis.●Skilled in client communication to gather data and project requirements.●Set up and configured Azure pipelines for CI/CD of data pipelines.●Created and optimized MS-SQL tables for efficient data storage.●Populated metrics to Grafana Dashboard from manufacturing data.●Designed ETL pipelines for data extraction from various sources.●Applied statistical techniques and optimization for data contextualization.●Delivered pipelines for leakage, defect detection, and insights.Key Achievement -With accommodating data from multiple sources makes it easy to contextualize data for easy analysis purposes, which improves the manufacturing lines are more adhered to not making defects/leakages in product manufacturing. The defects/leakages have been reduced to half of the previous defects identified.Software Engineer and Data Analystin Larsen and Toubro Technological Servicesworked in Oil & GasDomainJan 2020 –Oct 2021(Role –Data Engineer, Developer)Vadodara(Python, NumPy, Pandas, SciPy, MS SQL Server 2017, Oracle 11Gand Oracle 10G, xlrd, xlwt, openpyxl (Anaconda Navigator Framework)).Plant Portal Data Analysis [ Covestro, IPEP (Data analytics team)]●Implementation  for  the  process  of  Analysis  (Deduplication,  Data  Seagration,  File  Reference  Missing  in Smart  Plantportal,  finding  different equipment properties in plant portal, Different levels of data missing. Different file formats, Segregationand separation of different language formats, metadata, and migrated documents in smart plant portal). ●Used Task, Thread and Parallel process and optimizedcodes with Singleton and Factory Design pattern.Excel dynamic read and write with formattingusing closedXML, OpenXML, EPPlus, Excel Data Reader.●Key Achievement –Computation has reduced time significantly for a large dataset and customer delivery satisfaction was achieved on-time.SPI (Smart PlantIntegration) and SPEC [ Covestro, IPEP (Data analytics team)]●Data Comparison of migrating data from oracle to SQL Server. Generation of Excel Data and its comparison using EPPlus and generating reports. Created Custom controls and User controls, Delegation of custom and user controls, Background task and multithreading with parameterized arguments, async and await tasks, a comparison using multi and different primary keys also with dynamic and different key columns. Data view sync. Comparison checks for BLOB data and other Hash File data.●Implemented Task, Thread and Parallel process and optimizedcodes with Singleton and Factory Design pattern.Excel dynamic read and write with formattingusing closedXML, EPPlus, ExcelDataReader and Dataset in Visual Studio. Logging facility in python and used data frames.●Key Achievement –With multithreading and parallel processing output time has been significantly improved by more than 90%. Worked closely with cross-functional teams including plant engineers, data owners and site engineers to design, build and deliver end-to end solutions to their individual data conversion and wrangling problems.Software Engineerin Quantum Mutual Fundworked in FinancialDomainAug 2018 -Jan 2020(Role –Full Stack Developer/ Data Engineer)MumbaInvest Online (INON) -Backend Developer●Developed business logic for investment in Q-MF & transaction, Re-Purchase, Redemption, Switch, New Purchase, STP, SWP, SIP.●Optimized backend SQL code for Investors, usedto do KYC according to their citizenship with different criteria & process form in different queues.Software Engineer in Smart Solutionsworked in CRM, FMCG DomainOct 2013 -July 2018(Role –Full Stack Developer)MumbaiInventory management and services -Backend Developer ●Capturing, managing, and maintaining inventories and managing. Assigningblocks and holdinginventories.Graduate Engineer Trainee in Godrej & Boyce worked in CRM DomainOct 2012 -Sept 2013(Role –Full Stack Developer)MumbaiTimespan-Attendance and Leave management system, Assets management system & Drawer depository safe -Application ●Capturing, managing, and maintaining attendance and leaves for the employee in the company. Migrating assets and system information to standard format.CERTIFICATION AND LINKS(Recent)●LinkedIn & Pluralsight Certificates -#Using Python with Excel#Data Wrangling with Pandas for DataEngineers\u001b[0m\u001b[32;1m\u001b[1;3mBased on the retrieved document, it appears that Krishna was working as a Senior Data Engineer in Capgemini from October 2021 to present, and prior to that, he worked in various roles in different companies including Larsen and Toubro Technological Services, Quantum Mutual Fund, Smart Solutions, and Godrej & Boyce, starting from October 2012.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Retrieve relevant documents for: where Krishna was working on 2012',\n",
       " 'output': 'Based on the retrieved document, it appears that Krishna was working as a Senior Data Engineer in Capgemini from October 2021 to present, and prior to that, he worked in various roles in different companies including Larsen and Toubro Technological Services, Quantum Mutual Fund, Smart Solutions, and Godrej & Boyce, starting from October 2012.'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools = [retrieve_documents]\n",
    "query = \"where Krishna was working on 2012\"\n",
    "\n",
    "agent = create_tool_calling_agent(llm, tools, prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "agent_executor.invoke({\"input\": f\"Retrieve relevant documents for: {query}\"})\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_kri_mes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
